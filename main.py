import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from supabase import create_client, Client
import os
from dotenv import load_dotenv

# Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LinearRegression
from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, roc_auc_score
from sklearn.model_selection import train_test_split, GridSearchCV

# --- CONFIGURATION SUPABASE ---
# Charger les variables d'environnement depuis .env
load_dotenv()

URL = os.getenv("SUPABASE_URL")
KEY = os.getenv("SUPABASE_KEY")
supabase: Client = create_client(URL, KEY)

# ==========================================
# UTILITAIRES
# ==========================================

def get_data():
    """R√©cup√®re les donn√©es de Supabase."""
    print("üì° R√©cup√©ration des donn√©es depuis Supabase...")
    response = supabase.table("ai_training_data").select("*").execute()
    df = pd.DataFrame(response.data)
    return df

def clean_data(df):
    """Nettoie les donn√©es (erreurs de types, dates dans colonnes num√©riques)."""
    print("üßπ Nettoyage des donn√©es...")
    
    # Conversion date
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    
    # Colonnes num√©riques critiques
    numeric_cols = [
        'cote_dom_clean', 'cote_nul_clean', 'cote_ext_clean',
        'home_forme_pts_last5', 'home_moy_buts_marques_last5', 'home_moy_buts_encaisse_last5',
        'away_forme_pts_last5', 'away_moy_buts_marques_last5', 'away_moy_buts_encaisse_last5'
    ]
    
    # Force la conversion en num√©rique
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Supprime les lignes avec des NaN
    initial_len = len(df)
    df = df.dropna(subset=numeric_cols + ['ftr'])
    print(f"   üìâ Lignes supprim√©es (erreurs/dates) : {initial_len - len(df)}")
    print(f"   ‚úÖ Lignes restantes : {len(df)}")
    
    return df

def traduire_resultat(code):
    """Traduit H/D/A en texte lisible."""
    if code == 'H': return "Victoire Domicile üè†"
    if code == 'A': return "Victoire Ext√©rieur ‚úàÔ∏è"
    return "Match Nul ü§ù"

def plot_confusion_matrix(y_true, y_pred, labels, title, filename):
    """G√©n√®re et sauvegarde la matrice de confusion."""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=labels, yticklabels=labels)
    plt.xlabel('Pr√©diction')
    plt.ylabel('R√©alit√©')
    plt.title(title)
    plt.savefig(filename)
    plt.close()
    print(f"üñºÔ∏è Matrice sauvegard√©e sous : {filename}")

# ==========================================
# 1. CLASSIFICATION (Random Forest & SVM)
# ==========================================

# N'oublie pas d'importer GridSearchCV si tu veux aller plus loin, 
# mais ici on fait un r√©glage manuel optimis√©.

def run_classification(df):
    print("\nü§ñ --- D√âBUT CLASSIFICATION OPTIMIS√âE ---")
    
    features = [
        'cote_dom_clean', 'cote_nul_clean', 'cote_ext_clean',
        'home_forme_pts_last5', 'away_forme_pts_last5',
        'home_moy_buts_marques_last5', 'away_moy_buts_encaisse_last5'
    ]
    
    X = df[features]
    y = df['ftr']
    
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    labels = le.classes_
    
    # On garde 20% pour le test
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    print("\nüîç Recherche des meilleurs hyperparam√®tres (GridSearch)...")
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [5, 10, None],
        'min_samples_split': [2, 5, 10],
        'class_weight': ['balanced', None]
    }
    
    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='roc_auc_ovr')
    grid_search.fit(X_train, y_train)
    
    print(f"üèÜ Meilleurs param√®tres trouv√©s : {grid_search.best_params_}")
        
    rf_model = grid_search.best_estimator_
    # Le reste (predict, proba...) reste identique
    
    # --- AM√âLIORATION 1 : RANDOM FOREST TUN√â ---
    print("\nüå≤ Entra√Ænement Random Forest (Optimis√©)...")
    
    # class_weight='balanced' : Force le mod√®le √† pr√™ter attention aux Nuls
    # max_depth=10 : Emp√™che le "par c≈ìur" (overfitting)
    # n_estimators=200 : Plus d'arbres pour plus de stabilit√©
    rf_model = RandomForestClassifier(
        n_estimators=200, 
        max_depth=10, 
        min_samples_leaf=4,
        class_weight='balanced', 
        random_state=42
    )
    
    rf_model.fit(X_train, y_train)
    y_pred_rf = rf_model.predict(X_test)
    proba_rf = rf_model.predict_proba(X_test)
    
    print("üìä R√©sultats Random Forest :")
    print(classification_report(y_test, y_pred_rf, target_names=labels))
    plot_confusion_matrix(y_test, y_pred_rf, labels, "Matrice RF Optimis√©e", "confusion_matrix_rf.png")
    
    # --- AM√âLIORATION 2 : SVM TUN√â ---
    print("\nüõ°Ô∏è Entra√Ænement SVM (Optimis√©)...")
    # C=1.0 et kernel rbf sont standard, mais class_weight aide aussi ici
    svm_model = SVC(kernel='rbf', C=1.0, probability=True, class_weight='balanced', random_state=42)
    svm_model.fit(X_train_scaled, y_train)
    y_pred_svm = svm_model.predict(X_test_scaled)
    
    print("üìä R√©sultats SVM :")
    print(classification_report(y_test, y_pred_svm, target_names=labels))
    plot_confusion_matrix(y_test, y_pred_svm, labels, "Matrice SVM Optimis√©e", "confusion_matrix_svm.png")

    print("‚úÖ Mod√®les entra√Æn√©s.")

    try:
        auc_score = roc_auc_score(y_test, proba_rf, multi_class='ovr') 
        print(f"üåü Score AUC Global (Random Forest) : {auc_score:.4f}")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur calcul AUC: {e}")

    return rf_model, svm_model, scaler, le


# ==========================================
# 2. R√âGRESSION LIN√âAIRE (√âvolution des cotes)
# ==========================================

def run_regression(df, team_name):
    print(f"\nüìà --- D√âBUT R√âGRESSION LIN√âAIRE ({team_name}) ---")
    
    # Filtrer l'√©quipe
    team_df = df[(df['hometeam'] == team_name) | (df['awayteam'] == team_name)].copy()
    
    if len(team_df) < 10:
        print(f"‚ùå Pas assez de donn√©es pour {team_name}. Essaie une autre √©quipe.")
        return

    # Pr√©parer la cote √† analyser
    team_df['ma_cote'] = np.where(team_df['hometeam'] == team_name, 
                                  team_df['cote_dom_clean'], 
                                  team_df['cote_ext_clean'])
    
    team_df = team_df.sort_values('date')
    team_df['time_index'] = np.arange(len(team_df))
    
    X = team_df[['time_index']]
    y = team_df['ma_cote']
    
    # Entra√Ænement
    reg = LinearRegression()
    reg.fit(X, y)
    y_pred = reg.predict(X)
    
    # Analyse
    coef = reg.coef_[0]
    tendance = "en hausse ‚ÜóÔ∏è" if coef > 0 else "en baisse ‚ÜòÔ∏è"
    msg_confiance = "(L'√©quipe est moins favorite)" if coef > 0 else "(L'√©quipe est plus favorite)"
    
    print(f"Coefficient (Pente) : {coef:.4f}")
    print(f"üëâ Les cotes de {team_name} sont globalement {tendance} {msg_confiance}.")

    # Sauvegarde du Graphique
    plt.figure(figsize=(10, 6))
    plt.scatter(team_df['date'], y, color='blue', alpha=0.4, label='Cotes r√©elles')
    plt.plot(team_df['date'], y_pred, color='red', linewidth=2, label='Tendance')
    plt.title(f"√âvolution des cotes de victoire : {team_name}")
    plt.xlabel("Ann√©es")
    plt.ylabel("Cote")
    plt.legend()
    
    filename = f"regression_{team_name}.png"
    plt.savefig(filename)
    plt.close()
    print(f"üñºÔ∏è Graphique sauvegard√© sous : {filename}")

# ==========================================
# 3. PR√âDICTION (INFERENCE)
# ==========================================

def predire_un_match(rf_model, svm_model, scaler, le, match_data):
    print(f"\nüîÆ --- PR√âDICTION : {match_data['hometeam']} vs {match_data['awayteam']} ---")

    # Colonnes EXACTEMENT comme √† l'entra√Ænement
    features_names = [
        'cote_dom_clean', 'cote_nul_clean', 'cote_ext_clean',
        'home_forme_pts_last5', 'away_forme_pts_last5',
        'home_moy_buts_marques_last5', 'away_moy_buts_encaisse_last5'
    ]
    
    # Cr√©ation DataFrame
    input_data = pd.DataFrame([match_data], columns=['hometeam', 'awayteam'] + features_names)
    X_new = input_data[features_names]

    # Scaling pour SVM
    X_new_scaled = scaler.transform(X_new)

    # --- Pr√©diction RF ---
    prediction_rf = rf_model.predict(X_new)
    proba_rf = rf_model.predict_proba(X_new)
    resultat_rf = le.inverse_transform(prediction_rf)[0]
    
    print(f"\nüå≤ Avis du Random Forest :")
    print(f"üëâ R√©sultat pr√©vu : {traduire_resultat(resultat_rf)}")
    print(f"üìä Confiance :")
    for i, classe in enumerate(le.classes_):
        print(f"   - {traduire_resultat(classe)} : {proba_rf[0][i]*100:.1f}%")

    # --- Pr√©diction SVM ---
    prediction_svm = svm_model.predict(X_new_scaled)
    resultat_svm = le.inverse_transform(prediction_svm)[0]
    print(f"\nüõ°Ô∏è Avis du SVM : {traduire_resultat(resultat_svm)}")

# ==========================================
# MAIN EXECUTION
# ==========================================
if __name__ == "__main__":
    # 1. Chargement & Nettoyage
    df = get_data()
    df_clean = clean_data(df)
    
    # 2. Entra√Ænement et r√©cup√©ration des objets
    rf_model, svm_model, scaler, le = run_classification(df_clean)
    
    # 3. R√©gression (On utilise une √©quipe avec bcp de donn√©es)
    run_regression(df_clean, "Paris SG")
    
    # 4. Exemple de Pr√©diction (PSG vs OM)
    prochain_match = {
        'hometeam': 'Paris SG',
        'awayteam': 'Marseille',
        'cote_dom_clean': 1.55,       # PSG Favori
        'cote_nul_clean': 4.20,
        'cote_ext_clean': 6.00,
        'home_forme_pts_last5': 12,   # Bonne forme
        'away_forme_pts_last5': 8,    # Forme moyenne
        'home_moy_buts_marques_last5': 2.2,
        'away_moy_buts_encaisse_last5': 1.1
    }
    
    predire_un_match(rf_model, svm_model, scaler, le, prochain_match)