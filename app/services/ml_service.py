"""Service pour les mod√®les de machine learning."""
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # Backend non-interactif pour la production
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LinearRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from typing import Tuple, Dict, Any, Optional
import os
import pickle
from pathlib import Path

from app.config import settings

class MLService:
    """Service pour l'entra√Ænement et l'utilisation des mod√®les ML."""
    
    # Dossier pour sauvegarder les mod√®les
    MODELS_DIR = Path("models")
    
    def __init__(self):
        """Initialise le service ML et charge les mod√®les s'ils existent."""
        self.rf_model: Optional[RandomForestClassifier] = None
        self.svm_model: Optional[SVC] = None
        self.scaler: Optional[StandardScaler] = None
        self.label_encoder: Optional[LabelEncoder] = None
        self.features = [
            'cote_dom_clean', 'cote_nul_clean', 'cote_ext_clean',
            'home_forme_pts_last5', 'away_forme_pts_last5',
            'home_moy_buts_marques_last5', 'away_moy_buts_encaisse_last5'
        ]
        
        # Cr√©er le dossier models s'il n'existe pas
        self.MODELS_DIR.mkdir(exist_ok=True)
        
        # Charger automatiquement les mod√®les s'ils existent
        self.load_models()
    
    def is_trained(self) -> bool:
        """V√©rifie si les mod√®les sont entra√Æn√©s."""
        return (
            self.rf_model is not None and 
            self.svm_model is not None and 
            self.scaler is not None and 
            self.label_encoder is not None
        )
    
    def save_models(self) -> bool:
        """Sauvegarde les mod√®les entra√Æn√©s."""
        if not self.is_trained():
            return False
        
        try:
            # Sauvegarder chaque composant
            with open(self.MODELS_DIR / "rf_model.pkl", "wb") as f:
                pickle.dump(self.rf_model, f)
            with open(self.MODELS_DIR / "svm_model.pkl", "wb") as f:
                pickle.dump(self.svm_model, f)
            with open(self.MODELS_DIR / "scaler.pkl", "wb") as f:
                pickle.dump(self.scaler, f)
            with open(self.MODELS_DIR / "label_encoder.pkl", "wb") as f:
                pickle.dump(self.label_encoder, f)
            
            return True
        except Exception as e:
            print(f"Erreur lors de la sauvegarde des mod√®les : {e}")
            return False
    
    def load_models(self) -> bool:
        """Charge les mod√®les sauvegard√©s."""
        try:
            rf_path = self.MODELS_DIR / "rf_model.pkl"
            svm_path = self.MODELS_DIR / "svm_model.pkl"
            scaler_path = self.MODELS_DIR / "scaler.pkl"
            encoder_path = self.MODELS_DIR / "label_encoder.pkl"
            
            # V√©rifier que tous les fichiers existent
            if not all([rf_path.exists(), svm_path.exists(), scaler_path.exists(), encoder_path.exists()]):
                return False
            
            # Charger chaque composant
            with open(rf_path, "rb") as f:
                self.rf_model = pickle.load(f)
            with open(svm_path, "rb") as f:
                self.svm_model = pickle.load(f)
            with open(scaler_path, "rb") as f:
                self.scaler = pickle.load(f)
            with open(encoder_path, "rb") as f:
                self.label_encoder = pickle.load(f)
            
            return True
        except Exception as e:
            print(f"Erreur lors du chargement des mod√®les : {e}")
            return False
    
    def train_classification_models(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Entra√Æne les mod√®les de classification."""
        X = df[self.features]
        y = df['ftr']
        
        # Encodage de la cible
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(y)
        labels = self.label_encoder.classes_
        
        # Split des donn√©es
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, 
            test_size=settings.TEST_SIZE, 
            random_state=settings.RANDOM_STATE
        )
        
        # Scaling pour SVM
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # GridSearch pour Random Forest
        param_grid = {
            'n_estimators': [100, 200],
            'max_depth': [5, 10, None],
            'min_samples_split': [2, 5, 10],
            'class_weight': ['balanced', None]
        }
        
        grid_search = GridSearchCV(
            RandomForestClassifier(random_state=settings.RANDOM_STATE), 
            param_grid, 
            cv=3, 
            scoring='roc_auc_ovr'
        )
        grid_search.fit(X_train, y_train)
        best_params = grid_search.best_params_
        
        # Entra√Ænement Random Forest avec param√®tres optimis√©s
        self.rf_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            min_samples_leaf=4,
            class_weight='balanced',
            random_state=settings.RANDOM_STATE
        )
        self.rf_model.fit(X_train, y_train)
        y_pred_rf = self.rf_model.predict(X_test)
        proba_rf = self.rf_model.predict_proba(X_test)
        
        # Entra√Ænement SVM
        self.svm_model = SVC(
            kernel='rbf', 
            C=1.0, 
            probability=True, 
            class_weight='balanced', 
            random_state=settings.RANDOM_STATE
        )
        self.svm_model.fit(X_train_scaled, y_train)
        y_pred_svm = self.svm_model.predict(X_test_scaled)
        
        # Calcul des m√©triques
        rf_metrics = self._calculate_metrics(y_test, y_pred_rf, labels)
        svm_metrics = self._calculate_metrics(y_test, y_pred_svm, labels)
        
        # Calcul AUC
        auc_score = None
        try:
            auc_score = roc_auc_score(y_test, proba_rf, multi_class='ovr')
        except Exception:
            pass
        
        # G√©n√©ration des matrices de confusion
        self._plot_confusion_matrix(y_test, y_pred_rf, labels, "Matrice RF Optimis√©e", "confusion_matrix_rf.png")
        self._plot_confusion_matrix(y_test, y_pred_svm, labels, "Matrice SVM Optimis√©e", "confusion_matrix_svm.png")
        
        # Sauvegarder automatiquement les mod√®les apr√®s l'entra√Ænement
        self.save_models()
        
        return {
            'random_forest': rf_metrics,
            'svm': svm_metrics,
            'best_params': best_params,
            'auc_score': auc_score
        }
    
    def _calculate_metrics(self, y_true, y_pred, labels) -> Dict[str, Any]:
        """Calcule les m√©triques de classification."""
        # Calcul des m√©triques par classe
        precision_per_class = precision_score(y_true, y_pred, labels=range(len(labels)), average=None, zero_division=0)
        recall_per_class = recall_score(y_true, y_pred, labels=range(len(labels)), average=None, zero_division=0)
        f1_per_class = f1_score(y_true, y_pred, labels=range(len(labels)), average=None, zero_division=0)
        
        return {
            'accuracy': float(accuracy_score(y_true, y_pred)),
            'precision': {label: float(precision_per_class[i]) for i, label in enumerate(labels)},
            'recall': {label: float(recall_per_class[i]) for i, label in enumerate(labels)},
            'f1_score': {label: float(f1_per_class[i]) for i, label in enumerate(labels)},
            'confusion_matrix': confusion_matrix(y_true, y_pred).tolist(),
            'labels': labels.tolist()
        }
    
    def _plot_confusion_matrix(self, y_true, y_pred, labels, title, filename):
        """G√©n√®re et sauvegarde la matrice de confusion."""
        cm = confusion_matrix(y_true, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=labels, yticklabels=labels)
        plt.xlabel('Pr√©diction')
        plt.ylabel('R√©alit√©')
        plt.title(title)
        plt.savefig(filename)
        plt.close()
    
    def predict_match(self, match_data: Dict[str, Any]) -> Dict[str, Any]:
        """Pr√©dit le r√©sultat d'un match."""
        if not self.is_trained():
            raise ValueError(
                "Les mod√®les doivent √™tre entra√Æn√©s avant de faire des pr√©dictions. "
                "Utilisez l'endpoint POST /train pour entra√Æner les mod√®les."
            )
        
        # Cr√©ation DataFrame
        input_data = pd.DataFrame([match_data], columns=['hometeam', 'awayteam'] + self.features)
        X_new = input_data[self.features]
        
        # Scaling pour SVM
        X_new_scaled = self.scaler.transform(X_new)
        
        # Pr√©diction RF
        prediction_rf = self.rf_model.predict(X_new)
        proba_rf = self.rf_model.predict_proba(X_new)
        resultat_rf = self.label_encoder.inverse_transform(prediction_rf)[0]
        
        # Pr√©diction SVM
        prediction_svm = self.svm_model.predict(X_new_scaled)
        resultat_svm = self.label_encoder.inverse_transform(prediction_svm)[0]
        
        # Formatage des probabilit√©s
        probabilities = {}
        for i, classe in enumerate(self.label_encoder.classes_):
            probabilities[classe] = float(proba_rf[0][i])
        
        return {
            'random_forest': {
                'prediction': resultat_rf,
                'probabilities': probabilities,
                'prediction_text': self._traduire_resultat(resultat_rf)
            },
            'svm': {
                'prediction': resultat_svm,
                'prediction_text': self._traduire_resultat(resultat_svm)
            }
        }
    
    def _traduire_resultat(self, code: str) -> str:
        """Traduit H/D/A en texte lisible."""
        if code == 'H': return "Victoire Domicile üè†"
        if code == 'A': return "Victoire Ext√©rieur ‚úàÔ∏è"
        return "Match Nul ü§ù"
    
    def analyze_regression(self, df: pd.DataFrame, team_name: str) -> Dict[str, Any]:
        """Analyse l'√©volution des cotes d'une √©quipe."""
        # Filtrer l'√©quipe
        team_df = df[(df['hometeam'] == team_name) | (df['awayteam'] == team_name)].copy()
        
        if len(team_df) < 10:
            raise ValueError(f"Pas assez de donn√©es pour {team_name}. Minimum 10 matchs requis.")
        
        # Pr√©parer la cote √† analyser
        team_df['ma_cote'] = np.where(
            team_df['hometeam'] == team_name, 
            team_df['cote_dom_clean'], 
            team_df['cote_ext_clean']
        )
        
        team_df = team_df.sort_values('date')
        team_df['time_index'] = np.arange(len(team_df))
        
        X = team_df[['time_index']]
        y = team_df['ma_cote']
        
        # Entra√Ænement
        reg = LinearRegression()
        reg.fit(X, y)
        y_pred = reg.predict(X)
        
        # Analyse
        coef = float(reg.coef_[0])
        tendance = "en hausse ‚ÜóÔ∏è" if coef > 0 else "en baisse ‚ÜòÔ∏è"
        msg_confiance = "(L'√©quipe est moins favorite)" if coef > 0 else "(L'√©quipe est plus favorite)"
        
        # Sauvegarde du graphique
        filename = f"regression_{team_name.replace(' ', '_')}.png"
        plt.figure(figsize=(10, 6))
        plt.scatter(team_df['date'], y, color='blue', alpha=0.4, label='Cotes r√©elles')
        plt.plot(team_df['date'], y_pred, color='red', linewidth=2, label='Tendance')
        plt.title(f"√âvolution des cotes de victoire : {team_name}")
        plt.xlabel("Ann√©es")
        plt.ylabel("Cote")
        plt.legend()
        plt.savefig(filename)
        plt.close()
        
        return {
            'team_name': team_name,
            'coefficient': coef,
            'trend': tendance,
            'message': f"Les cotes de {team_name} sont globalement {tendance} {msg_confiance}.",
            'data_points': len(team_df),
            'chart_filename': filename
        }
    
    def analyze_feature_importance(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyse l'importance des features."""
        feature_names_clean = [
            'Cote Domicile', 
            'Cote Nul', 
            'Cote Ext√©rieur',
            'Forme Dom (5 derniers)', 
            'Forme Ext (5 derniers)',
            'Attaque Domicile', 
            'D√©fense Ext√©rieur'
        ]
        
        X = df[self.features]
        y = df['ftr']
        
        # Encodage de la cible
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        
        # Entra√Ænement Random Forest
        rf_model = RandomForestClassifier(
            n_estimators=200, 
            max_depth=5, 
            random_state=settings.RANDOM_STATE
        )
        rf_model.fit(X, y_encoded)
        
        # Extraction des importances
        importances = rf_model.feature_importances_
        
        # Cr√©ation d'un DataFrame pour trier les r√©sultats
        feature_imp_df = pd.DataFrame({
            'Variable': feature_names_clean,
            'Importance': importances
        }).sort_values(by='Importance', ascending=False)
        
        # Visualisation
        filename = "feature_importance_analysis.png"
        plt.figure(figsize=(10, 6))
        sns.barplot(
            x='Importance', 
            y='Variable', 
            data=feature_imp_df, 
            palette='viridis',
            hue='Variable',
            legend=False
        )
        plt.title('Importance des Features (Poids dans la d√©cision)', fontsize=15)
        plt.xlabel('Importance (0 √† 1)', fontsize=12)
        plt.ylabel('Variables', fontsize=12)
        plt.grid(axis='x', linestyle='--', alpha=0.7)
        plt.savefig(filename)
        plt.close()
        
        # Formatage des r√©sultats
        features_list = [
            {
                'name': row['Variable'],
                'importance': float(row['Importance'])
            }
            for _, row in feature_imp_df.iterrows()
        ]
        
        return {
            'features': features_list,
            'chart_filename': filename
        }

